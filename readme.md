# Astro-Gemini | Local/Offline LLM Chat 

Hi folks, decided to understand better the **Astro Framework**, integrating with **React** parts and implement the new **Gemini LLM API's** that can run directly on the browser this repository come on. 

## To work with 
-  Minimal CPU & GPU resources | **Running on** 11400f + GTX 970 4GB VRAM
-  Will you need **Google Canary** and proceed with the below documentation for installing it - ***basic steps***
https://developer.chrome.com/docs/extensions/ai/prompt-api

```
1. yarn install 
2. yarn dev | build & preview 
3. open with Google Canary
```

**Functionalities**
- Basic chatting control, capable of storing and updating a large amount of messages without so much lagging. Uses **@nanostores** to handle the messages context. 
- Really good whats-app built-in chat with LLM, that provides a great context in coding, math and other topics (obviously limited capabilities) that can run offline. 
- Examples of how to well integrate **Astro** with **React** components. 

**Working GIF**

![astro-gemini-ezgif com-crop (1)](https://github.com/user-attachments/assets/50ddcb2b-e0ee-4ee4-898c-bf2d13a87a0a)
